{"cells":[{"cell_type":"markdown","source":["## **MOSAIC PS-2(MATHEMATICAL PROBLEM SOLVER)**\n","The PS centres around a multimodal problem, taking into account the integration and processing of multiple modalities including text and images combining computer vision, natural language processing, mathematical reasoning and deep learning."],"metadata":{"id":"3_IexVETh5AQ"}},{"cell_type":"markdown","source":["## **Pipeline Followed to achieve this task -**\n","\n","\n","1.   ***Using datasets***- A hugging face library to decode the arrow file and import the dataset first\n","\n","\n","2.   ***Computer Vision and image feature and information extraction-*** This was done taking into account the two major components of any image pertaining to a geometrical problem- a geometrical figure or area and the labellings corresponding to these figures(like a parallel line symbol, angle markings, named vertices, labelled side lengths, highlighted points,arcs etc)\n","\n","    For this purpose, one major task was ocr based text detection and other major task was the implementation of foundational CNN to detect and learn geometrical patterns in the images and augmenting both to a foundational attention layer along with the problem text to gain a complete context of the problem.\n","    \n","    Models used for the preproccessing problem analysing step:\n","\n","\n","    1.   EfficientNetB4(employed as a CNN model- finetuned on the dataset using **Lora**)\n","    2.   easyocr - For ocr based detection of text in the image(directly\n","         employed as a lightweight ocr model to preprocess the\n","         context to be sent to the reasoning model)\n","\n","\n","\n","\n","3.  ***Sequence to Sequence Natural Language Processing( To learn about the problem description alongside the image):-***\n","A Natural Language processing model was needed to be designed in order to process a sentence long text per image and problem- the problem statement or description. For this purpose the following model has been finetuned and trained on the given dataset:-\n","\n","\n","       1.   BERT(bert-base-uncased)-finetuned as a text encoder to convert\n","            the given problem description into semantic contextual\n","            embeddings in a word embedding space ultimately\n","            to be augmented in a fusion space along with\n","            image feature vector to interpret the combined\n","            context(finetuned using lora)\n","4. ***Fusion Layer:-***\n","A custom multihead Attention layer to combine the extracted visual features from the image, the ocr text, and the problem description embeddings(encoded) in an augmented space to combine all individual contexts into one wholesome interpretable context of the problem. This layer is trained on the given dataset to learn the overall context better(apart from the finetuning and training of individual context handling models).\n","\n","5. ***Reasoning model:-***\n","After first preprocessing the dataset and the problems to be in a stage to be fed to a reasoning model, taking into account the combined as well as individual contexts, structed prompts are generated to be given to the reasoning model based on which it reasons and predicts the correct answer(by learning the supervised dataset with correct answer labels(ground truth) being given and utilized to minimize the error.\n","For this purpose the following models have been finetuned, trained and utiized to predict the answer:-\n","\n","\n","    1. t5-small decoder:- The combined context and learnt embeddings generated by the\n","    Fusion layer is decoded and fed as text-based prompts\n","    to the model(the prompts so generated are also trained to\n","    be as accurate as possible via force teaching)\n","\n","    2. phi-1.5:- open-source large language model (LLM) developed by\n","    Microsoft Research as part of the Phi series of small,\n","    efficient language models.Easily finetunable and lightweight to\n","    be trained with the limited capacity of a T4-GPU\n","    (as against other large models like deepseek-math7b-instruct etc.)\n","\n","The models used above have been finetuned using Lora and mixed precision\n"],"metadata":{"id":"u4ed9sKKoRB5"}},{"cell_type":"code","source":["   #just a flowchart representation-kindly dont run this cell\n","\n","                   +-----------------+\n","                   |  Geometry Image |\n","                   +-----------------+\n","                            |\n","                            v\n","        +-------------------+--------------------+\n","        |                                        |\n","        v                                        v\n","+------------------+                   +-------------------+\n","|   EfficientNet-B4|                   |      EasyOCR      |\n","| (Visual Features) |                 | (Text from Image)  |\n","+------------------+                   +-------------------+\n","        |                                        |\n","        +-------------------+--------------------+\n","                            |\n","                            v\n","                +-------------------------+\n","                |  Visual Context Encoder |\n","                +-------------------------+\n","\n","                            ↑\n","                            |\n","                            ↓\n","           +-----------------------------+\n","           |  Problem Description (Text) |\n","           +-----------------------------+\n","                            |\n","                            v\n","                   +------------------+\n","                   |  BERT Text Encoder|\n","                   +------------------+\n","\n","                            ↓\n","                            ↓\n","       +---------------------------------------------+\n","       |         Fusion Layer (Multihead Attention)  |\n","       | Combines Visual, OCR, and Textual Contexts  |\n","       +---------------------------------------------+\n","                            |\n","                            v\n","              +------------------------------+\n","              |  Unified Multimodal Embedding|\n","              +------------------------------+\n","                            |\n","                            v\n","         +------------------------------------------+\n","         |    Reasoning Module (LLM Decoders)       |\n","         |    - T5-small                             |\n","         |    - Phi-1.5 (Lightweight Finetuned LLM) |\n","         +------------------------------------------+\n","                            |\n","                            v\n","                +------------------------+\n","                |  Final Answer Output   |\n","                +------------------------+\n"],"metadata":{"id":"trPVMGCsZJRj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":122557,"status":"ok","timestamp":1744216612319,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"ACv4krFjaq-q","outputId":"c72b222e-cd9f-4070-c824-fdb1b84fbba0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m848.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.50.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n","Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2025.3.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n","\u001b[0mCollecting easyocr\n","  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.14.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.1.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.1.0)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.3.30)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (24.2)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n","Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n","Successfully installed easyocr-1.7.2 ninja-1.11.1.4 pyclipper-1.3.0.post6 python-bidi-0.6.6\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"]}],"source":["!pip install torch\n","!pip install torchvision\n","!pip install peft\n","!pip install tqdm\n","!pip install PIL\n","!pip install easyocr\n","!pip install numpy"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4008,"status":"ok","timestamp":1744216678991,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"WGayKxKtboHY","outputId":"a36e5105-abd7-410c-ef49-32d4d2fc9776"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rgKExeuFbgoX","executionInfo":{"status":"ok","timestamp":1744216719042,"user_tz":-330,"elapsed":30111,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"}}},"outputs":[],"source":["import torch #importing the necessary modules\n","\n","import torch.nn as nn\n","\n","from torch.utils.data import DataLoader, random_split\n","\n","from torchvision import transforms\n","\n","from transformers import(\n","BertModel, BertTokenizer,\n","T5ForConditionalGeneration, T5Tokenizer,\n","AutoModelForCausalLM, AutoTokenizer,\n","BitsAndBytesConfig\n",")\n","\n","from peft import get_peft_model, LoraConfig, TaskType\n","\n","from datasets import load_from_disk\n","\n","from tqdm import tqdm\n","\n","from PIL import Image\n","\n","import easyocr\n","\n","import numpy as np\n","\n","import random\n","\n","from torchvision import models\n","\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1744125105771,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"eLUvVvV8bxNt","outputId":"e546d5f9-e780-44e7-973e-6372e9441d89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ddfdc6fe750>"]},"metadata":{},"execution_count":2}],"source":["#initializing the necessary values to be used throughout the program\n","DEVICE='cuda'\n","\n","BATCH_SIZE=1\n","\n","EPOCHS=1\n","\n","VAL_SPLIT=0.3\n","\n","SEED=42\n","\n","random.seed(SEED)\n","\n","torch.manual_seed(SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18258,"status":"ok","timestamp":1744125145482,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"z_263Xvib0vn","outputId":"c51c84ff-44de-42d9-c78e-eb96ad37058a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from peft import get_peft_model, LoraConfig\n","\n","class EfficientNetAdapter(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        base=models.efficientnet_b4(pretrained=True) #freezing the other pretrained layers\n","        self.features=base.features #adaptive features\n","        self.pool=nn.AdaptiveAvgPool2d(1) #adpative pooling\n","\n","    def forward(self,x):\n","        x=self.features(x)\n","        return self.pool(x)\n","\n","model=EfficientNetAdapter()\n","lora_config=LoraConfig(# definign lora config\n","        r=16,\n","        lora_alpha=32,\n","        target_modules=[\"fc1\",\"fc2\"], #fully connected classifier head layers to be trained\n","        lora_dropout=0.05,\n","        bias=\"none\",\n","        task_type=\"FEATURE_EXTRACTION\"\n","        )\n","model.features=get_peft_model(model.features,lora_config).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6290,"status":"ok","timestamp":1744125197668,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"kBqiWP4jb5rA","outputId":"685b6818-1347-4fe8-b157-c65b4cbf432c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["text_encoder=BertModel.from_pretrained(\"bert-base-uncased\")\n","\n","text_encoder=get_peft_model(text_encoder, LoraConfig(#finetuning bert-base encased using lora\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"query\",\"value\"], # Specifies the layers to apply LoRA fine-tuning.\n","    lora_dropout=0.05,\n","    task_type=TaskType.FEATURE_EXTRACTION\n",")).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwmfqC1Kb-iP"},"outputs":[],"source":["# This layer combines the extracted visual features from the image, the ocr text, and the problem description embeddings(encoded) in an augmented space to combine all individual contexts into\n","# one wholesome interpretable context of the problem using the technique of multihead attention(in a transformer)\n"," class FusionLayer(nn.Module):\n","    def __init__(self, img_dim=1792, text_dim=768):\n","        super().__init__()\n","        self.img_proj = nn.Linear(img_dim, 512)\n","        self.text_proj = nn.Linear(text_dim, 512)\n","        self.attention = nn.MultiheadAttention(512, 8, batch_first=True) #defining the multihead attention layer\n","        self.output_proj = nn.Linear(512, 2048)  # New projection layer\n","\n","    def forward(self, img_feats, text_feats):\n","        img_proj = self.img_proj(img_feats) #image and text projected to embeddings of dimensions 512\n","        text_proj = self.text_proj(text_feats)\n","        img_proj = img_proj.unsqueeze(1)\n","        text_proj = text_proj.unsqueeze(1)\n","        attn_out, _ = self.attention(img_proj, text_proj, text_proj)\n","        out = attn_out.squeeze(1)               # [B, 512]\n","        out = self.output_proj(out)             # [B, 2048]\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyTZjWkxcCKp"},"outputs":[],"source":["t5_decoder= T5ForConditionalGeneration.from_pretrained(\n","    \"t5-small\",\n",").to(DEVICE)\n","\"\"\"\n","This decoder is defined to decode the embeddings finally generated by the fusion layer after combining the visual and text based context into text, which\n","is to be fed to the phi-1.5 reasoning model as prompt to predict the answer.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10904,"status":"ok","timestamp":1744125337635,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"3a5vmIxPcGH0","outputId":"3da4f753-0480-4dc5-b6e7-6d3c716fdd01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model on device: cuda:0\n"]}],"source":["import torch\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n","from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n","\n","# Step 1: Load config and tokenizer\n","config = AutoConfig.from_pretrained(\"microsoft/phi-1.5\")\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1.5\")\n","\n","# Step 2: Define model\n","phi_model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/phi-1.5\",\n","    config=config,\n","    trust_remote_code=True,\n","    use_safetensors=True,\n","    revision=\"main\",\n","    device_map='auto'\n",")\n","\n","# Step 3: Prepare for LoRA / PEFT\n","phi_model = prepare_model_for_kbit_training(phi_model)\n","\n","# Step 4: Add Lora Config\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"k_proj\"], #layers to be finetuned\n","    lora_dropout=0.05,\n","    task_type=\"CAUSAL_LM\",\n",")\n","phi_model = get_peft_model(phi_model, lora_config).to('cuda')\n","\n","# Step 4: Gradient checkpointing to reduce memory (still on CUDA)\n","phi_model.gradient_checkpointing_enable()\n","\n","# Confirm model is on CUDA\n","print(f\"Model on device: {next(phi_model.parameters()).device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aa8zhplucXJN"},"outputs":[],"source":["ocr_reader = easyocr.Reader(['en'], gpu=DEVICE == 'cuda')\n","\n","# Preprocessing function for images\n","def preprocess_image(image):\n","    if image.mode == 'RGBA':\n","        image = image.convert('RGB')\n","    return transforms.Compose([\n","        transforms.Resize((380, 380)),\n","        transforms.ToTensor()\n","    ])(image)\n","\n","# Dataset class\n","class GeometryDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset):  # initalizing dataset and bert-tokenizer for every instance\n","        self.dataset = dataset\n","        self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","    def __len__(self):  # calculating the length of the dataset for evry instance\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):  #\n","        item = self.dataset[idx]\n","\n","        # Preprocess image\n","        image = preprocess_image(item['images'][0])\n","\n","        # OCR from original image\n","        ocr_text = \" \".join([res[1] for res in ocr_reader.readtext(np.array(item['images'][0]))])\n","        cleaned_problem = item['problem'].replace(\"<image>\", \"\").strip()\n","\n","        # Combine all text\n","        full_text = f\"{cleaned_problem}\\nOCR: {ocr_text}\\nOptions: \" + \", \".join(\n","            [f\"{chr(65 + i)}: {choice}\" for i, choice in enumerate(item['choices'])]\n","        )\n","\n","        # BERT tokenization\n","        text_inputs = self.bert_tokenizer(\n","            full_text,\n","            return_tensors=\"pt\",\n","            max_length=512,\n","            padding=\"max_length\",\n","            truncation=True\n","        )\n","\n","        # Ground truth label\n","        answer_idx = ord(item['ground_truth'].upper()) - ord('A')\n","\n","        # Handle option_values only if numeric(for mse loss calculation in case of numeric values otherwise cosine similarity for string options(delat by raw_choices))\n","        try:\n","            option_values = torch.tensor([float(c) for c in item['choices']], dtype=torch.float32,requires_grad=False)\n","        except ValueError:\n","            option_values = None  # Handle later in loss function\n","        if image is None or text_inputs['input_ids'] is None or text_inputs['attention_mask'] is None or option_values is None or item['choices'] is None:\n","            return None\n","\n","        return {\n","            'input_ids': text_inputs['input_ids'].squeeze(0),\n","            'attention_mask': text_inputs['attention_mask'].squeeze(0),\n","            'image': image,\n","            'labels': torch.tensor(answer_idx),\n","            'option_values': option_values,\n","            'raw_choices': item['choices']  # Save raw choices for hybrid loss decision\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":308,"status":"ok","timestamp":1744127599089,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"tomD-Cnhcdyh","outputId":"b99f5675-f99c-48d7-d717-14ee6c1c6b16"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'transformers.tokenization_utils_base.BatchEncoding'>\n","{'input_ids': tensor([[ 101, 2054, 2003, 1016, 1009, 1016, 1029, 1037, 1012, 1016, 1038, 1012,\n","         1017, 1039, 1012, 1018, 1040, 1012, 1019,  102,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]])}\n"]}],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","#testing the tokenizer using custom input\n","text = \"What is 2 + 2? A. 2 B. 3 C. 4 D. 5\"\n","tokens = tokenizer(\n","    text,\n","    return_tensors=\"pt\",\n","    padding=\"max_length\",\n","    max_length=512,\n","    truncation=True\n",")\n","\n","print(type(tokens))\n","print(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6152,"status":"ok","timestamp":1744124090446,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"UkKXN_AfciR6","outputId":"ba2d568e-67a4-493c-c647-fe765251b69d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive #mount google drive to access the training dataset\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":932,"status":"ok","timestamp":1744127604857,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"5lmQtwxKdvFo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"487abb87-31da-40ae-c13a-57627ab1f484"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["from torch.utils.data.dataloader import default_collate\n","def collate_fn_skip_none(batch): #custom collate function to only allow tensors, numpy arrays and alllowed data structures to pass for each batch while training\n","    batch = [item for item in batch if item is not None]\n","    if len(batch) == 0:\n","        return None  # let the training loop handle this\n","    return default_collate(batch)\n","full_dataset=load_from_disk(\"/content/drive/My Drive/MOSAIC PS-2\")['train']\n","\n","train_size=int(len(full_dataset)*(1-VAL_SPLIT)) #training data size\n","\n","train_dataset, val_dataset=random_split(full_dataset,[train_size,len(full_dataset)-train_size]) #training and cross validation dataset split\n","\n","train_loader=DataLoader(GeometryDataset(train_dataset),batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn_skip_none) #loading training data\n","\n","val_loader=DataLoader(GeometryDataset(val_dataset),batch_size=BATCH_SIZE,collate_fn=collate_fn_skip_none) #loading cross validation data\n","\n","image_encoder=EfficientNetAdapter().to(DEVICE) #intializing image enocder-storing everything to 'cuda' to prevent backpropagation chain from breaking at any point\n","\n","fusion_layer=FusionLayer().to(DEVICE)# initializing fusion_layer-storing everything to 'cuda' to prevent backpropagation chain from breaking at any point\n","optimizer=torch.optim.AdamW([\n","    {'params': phi_model.parameters(), \"lr\": 1e-5},\n","    {'params':image_encoder.parameters(),'lr':1e-5}, #learning rate for params determined via hyperparameter tuning\n","    {'params':text_encoder.parameters(),'lr':2e-5},\n","    {'params':fusion_layer.parameters(),'lr':3e-5}\n","\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":922807,"status":"ok","timestamp":1744129661138,"user":{"displayName":"Juhi Saxena 4-Yr B.Tech.: Electronics Engg.,IIT(BHU","userId":"08919347091833229047"},"user_tz":-330},"id":"la3bJVnSeC06","outputId":"ad03ee19-0411-4e59-cae8-f9a20be6f3f6"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-13-03740e58c324>:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["Grad check - image_encoder: True\n","Grad check - text_encoder: True\n","Grad check - fusion_layer: True\n","Grad check - phi_model: True\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 2031/2031 [12:00<00:00,  2.82it/s, loss=inf, acc=29.83%]\n","Validation: 100%|██████████| 871/871 [02:25<00:00,  5.97it/s, loss=3.78e+3, acc=33.42%]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/1\n","Train Loss: inf | Acc: 29.83%\n","Val Loss: 3781.8049 | Acc: 33.42%\n","Training completed\n"]}],"source":["from torch.nn.functional import cosine_embedding_loss, gumbel_softmax\n","import torch.nn as nn\n","from torch import autocast\n","\n","ce_loss = nn.CrossEntropyLoss()\n","mse_loss = nn.MSELoss()\n","\n","# Convert parameters to float16 if needed(to prevent error in case any torch.uint8 param is passed)\n","for name, param in phi_model.named_parameters():\n","    if isinstance(param, torch.nn.Parameter) and param.dtype == torch.uint8:\n","        param.data = param.data.to(torch.float16)\n","\n","def run_epoch(model, loader, optimizer=None, is_train=True):\n","    scaler = torch.cuda.amp.GradScaler()# Initializes a gradient scaler for mixed precision training to improve performance and reduce memory usage on GPU\n","    total_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    model.train() if is_train else model.eval()\n","   # Initialize a progress bar for training or validation\n","progress = tqdm(loader, desc=\"Training\" if is_train else \"Validation\")\n","\n","# Enable or disable gradient computation based on whether it's training or validation\n","with torch.set_grad_enabled(is_train):\n","    for batch in progress:\n","        # Skip invalid batches\n","        if batch is None:\n","            continue\n","\n","        # Zero out gradients if in training mode\n","        if is_train:\n","            optimizer.zero_grad()\n","\n","        # Move images and other inputs to the specified device (GPU)\n","        images = batch['image'].to(DEVICE)\n","\n","        # Convert grayscale images to 3-channel (RGB) format if necessary\n","        if images.shape[1] == 1:\n","            images = images.repeat(1, 3, 1, 1)\n","\n","        input_ids = batch['input_ids'].to(DEVICE)  # Tokenized input IDs for text\n","        attention_mask = batch['attention_mask'].to(DEVICE)  # Attention mask for text\n","        answer_idx = batch['labels'].to(DEVICE)  # Ground truth answer indices\n","\n","        options = batch['option_values']  # Numeric values for multiple-choice options (if available)\n","        raw_choices = batch['raw_choices']  # Raw text of multiple-choice options\n","\n","        # Use mixed precision (autocast) for faster computation and reduced memory usage on GPUs\n","        with autocast(\"cuda\"):\n","            # === Feature Encoding ===\n","            # Extract image features using the image encoder and flatten them\n","            img_feats = image_encoder(images).view(images.size(0), -1)  # [Batch_Size, Image_Feature_Dim]\n","\n","            # Extract text features using the text encoder and compute mean pooling over token embeddings\n","            text_feats = text_encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            ).last_hidden_state.mean(1)  # [Batch_Size, Text_Feature_Dim]\n","\n","            # === Feature Fusion ===\n","            # Concatenate image and text features and project them into a joint feature space\n","            fused_feats = fusion_layer(img_feats, text_feats)\n","            fused_feats = fused_feats.unsqueeze(1)  # Add a sequence dimension for decoder input\n","\n","            # Prepare decoder input IDs (start token followed by ground truth answer index)\n","            decoder_start_token_id = tokenizer.pad_token_id  # Use padding token as start token\n","            decoder_input_ids = torch.cat(\n","                [\n","                    torch.full((answer_idx.size(0), 1), decoder_start_token_id, device=answer_idx.device),\n","                    answer_idx.unsqueeze(1)  # Add a dimension for sequence tokens\n","                ],\n","                dim=1\n","            )\n","            \"\"\"\n","            This step is done in order to decode the final visual+textual context embeddings generated by the fusion layer to be fed to the reasoning model\n","             as prompt to predict the answer.\n","            \"\"\"\n","\n","            # === Model Forward Pass ===\n","            outputs = model(\n","                inputs_embeds=fused_feats,  # Fused image-text embeddings as input\n","                attention_mask=attention_mask,  # Attention mask for the decoder\n","                decoder_input_ids=decoder_input_ids,  # Decoder input IDs (start token + ground truth)\n","                use_cache=False  # Disable caching during training/validation\n","            )\n","\n","            logits = outputs.logits[:, -1, :4]  # Extract logits for the last token (4 options)\n","\n","            # === Loss Calculation ===\n","            loss_ce = ce_loss(logits, answer_idx)  # Cross-entropy loss for classification\n","\n","            if options is not None:\n","                # If numeric option values are provided:\n","                options_tensor = options.to(DEVICE)  # Move option values to the device\n","                probs = torch.softmax(logits, dim=-1)  # Compute probabilities from logits\n","\n","                predicted_values = torch.matmul(probs, options_tensor.unsqueeze(-1)).squeeze()\n","                correct_values = options_tensor.gather(1, answer_idx.unsqueeze(1)).squeeze()\n","\n","                loss_hybrid = mse_loss(predicted_values, correct_values)  # Mean squared error loss for numeric predictions\n","\n","            else:\n","                # If numeric option values are not provided:\n","                choice_embeddings = []\n","                \"\"\"\n","                The cosine loss is calculated for predicted embeddings and correct embeddings to measure their similarity by first encoding the text via the same text_encoder(bert model)\n","                and then calculating the cosine similarity loss between the predicted and correct embeddings.\n","                \"\"\"\n","\n","                for choices in raw_choices:\n","                    tokens = text_encoder.tokenizer(\n","                        choices,\n","                        return_tensors=\"pt\",\n","                        padding=\"max_length\",\n","                        truncation=True,\n","                        max_length=32\n","                    ).to(DEVICE)\n","\n","                    emb = text_encoder(**tokens).last_hidden_state.mean(1)\n","                    choice_embeddings.append(emb)\n","\n","                choice_embeddings = torch.stack(choice_embeddings)\n","                probs = torch.softmax(logits, dim=-1).unsqueeze(-1)\n","\n","                pred_emb = (choice_embeddings * probs).sum(dim=1)\n","\n","                indices = answer_idx.unsqueeze(1).unsqueeze(2).expand(-1, 1, pred_emb.size(-1))\n","\n","                correct_emb = torch.gather(choice_embeddings, 1, indices).squeeze(1)\n","\n","                loss_hybrid = cosine_embedding_loss(\n","                    pred_emb,\n","                    correct_emb,\n","                    torch.ones(pred_emb.size(0)).to(DEVICE)\n","                )\n","\n","            loss = 0.4 * loss_ce + 0.6 * loss_hybrid  # Weighted combination of losses(weights determined by hyperparamter tuning)\n","\n","            if is_train:\n","                # Backward pass using GradScaler for mixed precision training\n","                scaler.scale(loss).backward()\n","                scaler.step(optimizer)\n","                scaler.update()\n","\n","            total_loss += loss.item()  # Accumulate total loss\n","\n","            _, predicted = torch.max(logits, 1)\n","\n","            correct += (predicted == answer_idx).sum().item()\n","\n","            total += answer_idx.size(0)\n","\n","            progress.set_postfix({\n","                'loss': total_loss / (progress.n + 1),\n","                'acc': f\"{correct / total:.2%}\"\n","            })\n","\n","    return total_loss / len(loader), correct / total\n","\n","\n","# === Training Loop ===\n","\n","best_val_acc = 0.0\n","EPOCHS = 1\n","\n","# Check if gradients are enabled for each component of the model pipeline\n","print(\"Grad check - image_encoder:\", any(p.requires_grad for p in image_encoder.parameters()))\n","print(\"Grad check - text_encoder:\", any(p.requires_grad for p in text_encoder.parameters()))\n","print(\"Grad check - fusion_layer:\", any(p.requires_grad for p in fusion_layer.parameters()))\n","print(\"Grad check - phi_model:\", any(p.requires_grad for p in phi_model.parameters()))\n","\n","for epoch in range(EPOCHS):\n","    train_loss, train_acc = run_epoch(phi_model, train_loader, optimizer)  # Train the model on training data\n","\n","    val_loss, val_acc = run_epoch(phi_model, val_loader, is_train=False)   # Evaluate the model on validation data\n","\n","    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n","    print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2%}\")\n","    print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%}\")\n","    \"\"\"\n","    The performance metric used here is accuracy:\n","     Accuracy measures the proportion of correctly predicted instances (both positive and negative) out of the total predictions made by the model\n","\n","    \"\"\"\n","\n","    if val_acc > best_val_acc:\n","        torch.save({\n","            'phi_model': phi_model.state_dict(),\n","            'image_encoder': image_encoder.state_dict(),\n","            'fusion_layer': fusion_layer.state_dict()\n","        }, \"best_model.pth\")   # Save the best model checkpoint\n","\n","        best_val_acc = val_acc\n","\n","print(\"Training completed\")\n","\n"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPh9nKf4ISJJ4nL0gKBG8uN"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}